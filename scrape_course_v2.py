#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Claude Code å®æˆ˜è¯¾ç¨‹å†…å®¹æŠ“å–è„šæœ¬ï¼ˆæ”¹è¿›ç‰ˆï¼‰
å®Œæ•´æŠ“å–æ‰€æœ‰ç« èŠ‚å†…å®¹å¹¶ä¿å­˜ä¸º Markdown æ–‡æ¡£
"""

import requests
from bs4 import BeautifulSoup
import os
import time
from urllib.parse import urljoin

# åŸºç¡€ URL
BASE_URL = "https://cholf5.com/claude-code-in-action/"

# æ‰€æœ‰ç« èŠ‚åˆ—è¡¨
CHAPTERS = [
    ("01", "å¼•è¨€", "01-introduction.html"),
    ("02", "ä»€ä¹ˆæ˜¯ç¼–ç åŠ©æ‰‹ï¼Ÿ", "02-what-is-a-coding-assistant.html"),
    ("03", "Claude Code å®æˆ˜", "03-claude-code-in-action.html"),
    ("04", "Claude Code å®‰è£…ä¸é…ç½®", "04-claude-code-setup.html"),
    ("05", "é¡¹ç›®å‡†å¤‡", "05-project-setup.html"),
    ("06", "æ·»åŠ ä¸Šä¸‹æ–‡", "06-adding-context.html"),
    ("07", "è¿›è¡Œä¿®æ”¹", "07-making-changes.html"),
    ("08", "è¯¾ç¨‹æ»¡æ„åº¦è°ƒæŸ¥", "08-course-satisfaction-survey.html"),
    ("09", "æ§åˆ¶ä¸Šä¸‹æ–‡", "09-controlling-context.html"),
    ("10", "è‡ªå®šä¹‰å‘½ä»¤", "10-custom-commands.html"),
    ("11", "Claude Code çš„ MCP æœåŠ¡å™¨", "11-mcp-servers-with-claude-code.html"),
    ("12", "GitHub é›†æˆ", "12-github-integration.html"),
    ("13", "è®¤è¯† Hooks", "13-introducing-hooks.html"),
    ("14", "å®šä¹‰ Hooks", "14-defining-hooks.html"),
    ("15", "å®ç°ä¸€ä¸ª Hook", "15-implementing-a-hook.html"),
    ("16", "Hooks å¸¸è§å‘ç‚¹", "16-gotchas-around-hooks.html"),
    ("17", "å®ç”¨çš„ Hooks", "17-useful-hooks.html"),
    ("18", "å¦ä¸€ä¸ªå®ç”¨ Hook", "18-another-useful-hook.html"),
    ("19", "Claude Code SDK", "19-the-claude-code-sdk.html"),
    ("20", "Claude Code æµ‹éªŒ", "20-quiz-on-claude-code.html"),
    ("21", "æ€»ç»“ä¸ä¸‹ä¸€æ­¥", "21-summary-and-next-steps.html")
]

def fetch_page(url):
    """è·å–é¡µé¢å†…å®¹"""
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        response.encoding = 'utf-8'
        return response.text
    except Exception as e:
        print(f"âŒ è·å–å¤±è´¥: {url} - {str(e)}")
        return None

def html_to_markdown(element, base_url=""):
    """å°†HTMLå…ƒç´ è½¬æ¢ä¸ºMarkdown"""
    if not element:
        return ""

    markdown_lines = []

    # å¤„ç†ä¸åŒçš„HTMLæ ‡ç­¾
    if element.name == 'h1':
        text = element.get_text().strip()
        markdown_lines.append(f"\n\n# {text}\n\n")
    elif element.name == 'h2':
        text = element.get_text().strip()
        markdown_lines.append(f"\n\n## {text}\n\n")
    elif element.name == 'h3':
        text = element.get_text().strip()
        markdown_lines.append(f"\n\n### {text}\n\n")
    elif element.name == 'h4':
        text = element.get_text().strip()
        markdown_lines.append(f"\n\n#### {text}\n\n")
    elif element.name == 'p':
        text = element.get_text().strip()
        if text:
            markdown_lines.append(f"{text}\n\n")
    elif element.name in ['ul', 'ol']:
        items = element.find_all('li', recursive=False)
        for li in items:
            text = li.get_text().strip()
            # ä¿ç•™åŠ ç²—æ ‡è®°
            for strong in li.find_all('strong'):
                text = text.replace(strong.get_text(), f"**{strong.get_text()}**")
            markdown_lines.append(f"- {text}\n")
        markdown_lines.append("\n")
    elif element.name == 'ol':
        items = element.find_all('li', recursive=False)
        for i, li in enumerate(items, 1):
            text = li.get_text().strip()
            # ä¿ç•™åŠ ç²—æ ‡è®°
            for strong in li.find_all('strong'):
                text = text.replace(strong.get_text(), f"**{strong.get_text()}**")
            markdown_lines.append(f"{i}. {text}\n")
        markdown_lines.append("\n")
    elif element.name == 'pre':
        code = element.get_text()
        markdown_lines.append(f"\n```\n{code}\n```\n\n")
    elif element.name == 'img':
        src = element.get('src', '')
        alt = element.get('alt', 'å›¾ç‰‡')
        markdown_lines.append(f"\n![{alt}]({src})\n\n")
    elif element.name == 'a':
        href = element.get('href', '')
        text = element.get_text().strip()
        if href and not href.startswith('javascript'):
            markdown_lines.append(f"[{text}]({href})")
    elif element.name == 'strong' or element.name == 'b':
        text = element.get_text().strip()
        markdown_lines.append(f"**{text}**")
    elif element.name == 'code':
        text = element.get_text().strip()
        markdown_lines.append(f"`{text}`")
    elif element.name == 'blockquote':
        text = element.get_text().strip()
        markdown_lines.append(f"\n> {text}\n\n")
    elif element.name == 'div' or element.name == 'section':
        # é€’å½’å¤„ç†å­å…ƒç´ 
        for child in element.children:
            if hasattr(child, 'name'):
                markdown_lines.append(html_to_markdown(child, base_url))

    return ''.join(markdown_lines)

def extract_content(html, chapter_num, chapter_title):
    """ä» HTML ä¸­æå–ä¸»è¦å†…å®¹"""
    soup = BeautifulSoup(html, 'html.parser')

    # æŸ¥æ‰¾å†…å®¹åŒºåŸŸ
    content_body = soup.find('div', class_='content-body')

    if not content_body:
        return None

    # æå–æ ‡é¢˜
    title_tag = content_body.find('h1')
    title = title_tag.get_text().strip() if title_tag else chapter_title

    # ç”ŸæˆMarkdown
    markdown_parts = []
    markdown_parts.append(f"\n\n# {title}\n\n")

    # å¤„ç†æ‰€æœ‰ç›´æ¥å­å…ƒç´ 
    for element in content_body.find_all(recursive=False):
        if element.name == 'h1':
            continue  # å·²ç»å¤„ç†è¿‡æ ‡é¢˜
        markdown_parts.append(html_to_markdown(element))

    return ''.join(markdown_parts)

def scrape_all_chapters():
    """æŠ“å–æ‰€æœ‰ç« èŠ‚"""
    all_content = []
    failed_chapters = []

    print("ğŸš€ å¼€å§‹æŠ“å– Claude Code å®æˆ˜è¯¾ç¨‹å†…å®¹...\n")

    for chapter_num, chapter_title, chapter_file in CHAPTERS:
        url = urljoin(BASE_URL, chapter_file)

        print(f"ğŸ“– æ­£åœ¨æŠ“å–: [{chapter_num}] {chapter_title}")

        html = fetch_page(url)
        if html:
            content = extract_content(html, chapter_num, chapter_title)
            if content:
                all_content.append(content)
                print(f"âœ… æˆåŠŸ: [{chapter_num}] {chapter_title}\n")
            else:
                failed_chapters.append((url, "å†…å®¹æå–å¤±è´¥"))
                print(f"âš ï¸  å†…å®¹æå–å¤±è´¥: [{chapter_num}] {chapter_title}\n")
        else:
            failed_chapters.append((url, "é¡µé¢è·å–å¤±è´¥"))
            print(f"âŒ å¤±è´¥: [{chapter_num}] {chapter_title}\n")

        # é¿å…è¯·æ±‚è¿‡å¿«
        time.sleep(0.5)

    # ç”Ÿæˆå®Œæ•´çš„ Markdown æ–‡æ¡£
    full_markdown = "# Claude Code å®æˆ˜è¯¾ç¨‹ï¼ˆä¸­æ–‡ç¿»è¯‘ï¼‰\n\n"
    full_markdown += "> **è¯¾ç¨‹è¯´æ˜**ï¼šæœ¬æ–‡æ¡£ç”±è‡ªåŠ¨åŒ–è„šæœ¬æŠ“å–ç”Ÿæˆ\n\n"
    full_markdown += "> **åŸè¯¾ç¨‹é“¾æ¥**ï¼šhttps://anthropic.skilljar.com/claude-code-in-action/303233\n\n"
    full_markdown += "> **å®˜æ–¹ç½‘ç«™**ï¼šhttps://cholf5.com/claude-code-in-action/index.html\n\n"
    full_markdown += "---\n\n"
    full_markdown += "## è¯¾ç¨‹ç›®å½•\n\n"

    for chapter_num, chapter_title, _ in CHAPTERS:
        full_markdown += f"{chapter_num}. [{chapter_title}](#ç« èŠ‚-{chapter_num})\n"

    full_markdown += "\n---\n\n"

    # æ·»åŠ æ‰€æœ‰ç« èŠ‚å†…å®¹
    for i, content in enumerate(all_content, 1):
        # æ·»åŠ ç« èŠ‚é”šç‚¹
        full_markdown += f'<a id="ç« èŠ‚-{i}"></a>\n\n'
        full_markdown += content
        full_markdown += "\n\n---\n\n"

    # ä¿å­˜æ–‡ä»¶
    output_file = "/Users/lartemacfiles/Desktop/VPT-åˆè¯Šæ•°æ®/Claude_Codeå®æˆ˜è¯¾ç¨‹_å®Œæ•´ç‰ˆ.md"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(full_markdown)

    print(f"\nğŸ“„ Markdown æ–‡æ¡£å·²ä¿å­˜: {output_file}")

    # ç”ŸæˆæŠ¥å‘Š
    report = f"""# Claude Code å®æˆ˜è¯¾ç¨‹æŠ“å–æŠ¥å‘Š

## ç»Ÿè®¡ä¿¡æ¯
- **æ€»ç« èŠ‚æ•°**: {len(CHAPTERS)}
- **æˆåŠŸæŠ“å–**: {len(all_content)} ç« 
- **å¤±è´¥ç« èŠ‚**: {len(failed_chapters)} ç« 
- **æˆåŠŸç‡**: {len(all_content)/len(CHAPTERS)*100:.1f}%

## è¯¾ç¨‹å¤§çº²
"""

    for chapter_num, chapter_title, chapter_file in CHAPTERS:
        report += f"**{chapter_num}. {chapter_title}**\n"
        report += f"   - æ–‡ä»¶: {chapter_file}\n"
        report += f"   - é“¾æ¥: {urljoin(BASE_URL, chapter_file)}\n\n"

    if failed_chapters:
        report += "\n## å¤±è´¥ç« èŠ‚\n\n"
        for url, reason in failed_chapters:
            report += f"- âŒ {url}\n"
            report += f"  åŸå› : {reason}\n\n"

    report += f"\n## è¾“å‡ºæ–‡ä»¶\n\n"
    report += f"- **Markdownæ–‡æ¡£**: `{output_file}`\n\n"
    report += f"ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n"

    report_file = "/Users/lartemacfiles/Desktop/VPT-åˆè¯Šæ•°æ®/å®æˆ˜è¯¾ç¨‹æŠ“å–æŠ¥å‘Š.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"ğŸ“Š æŠ“å–æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    print(f"\nâœ¨ æŠ“å–å®Œæˆ!")
    print(f"âœ… æˆåŠŸ: {len(all_content)} ç« ")
    if failed_chapters:
        print(f"âŒ å¤±è´¥: {len(failed_chapters)} ç« ")
    else:
        print(f"ğŸ‰ æ‰€æœ‰ç« èŠ‚å…¨éƒ¨æˆåŠŸæŠ“å–ï¼")

if __name__ == "__main__":
    scrape_all_chapters()
