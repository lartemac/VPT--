#!/usr/bin/env python3
"""
GLM-4.7 é€šç”¨åŠ©æ‰‹å·¥å…·
æ”¯æŒå¤šç§ä»»åŠ¡ï¼šæ–‡æœ¬ç”Ÿæˆã€ä»£ç ç¼–å†™ã€æ•°æ®åˆ†æã€åŒ»å­¦é—®ç­”ç­‰
"""

import sys
from zhipuai import ZhipuAI

# API é…ç½®
API_KEY = "232b1236880a4699957a592bed87aad2.3gYzmvvyIQN98DZb"

# æ¨¡å‹é…ç½®
MODELS = {
    "plus": "glm-4-plus",      # æœ€æ–°æœ€å¼ºï¼ˆGLM-4.7ï¼‰
    "flash": "glm-4-flash",    # å¿«é€Ÿç»æµ
    "air": "glm-4-air",        # è½»é‡çº§
    "standard": "glm-4"        # æ ‡å‡†ç‰ˆ
}

def chat_with_glm(prompt, model="plus", temperature=0.7, max_tokens=2000, system_prompt=None):
    """
    ä½¿ç”¨ GLM-4.7 è¿›è¡Œå¯¹è¯

    å‚æ•°ï¼š
        prompt: ç”¨æˆ·æç¤ºè¯
        model: æ¨¡å‹é€‰æ‹©ï¼ˆplus/flash/air/standardï¼‰
        temperature: æ¸©åº¦ï¼ˆ0-1ï¼Œè¶Šé«˜è¶Šåˆ›æ„ï¼‰
        max_tokens: æœ€å¤§è¾“å‡ºé•¿åº¦
        system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
    """

    model_name = MODELS.get(model, "glm-4-plus")

    # æ„å»ºæ¶ˆæ¯
    messages = []

    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})

    messages.append({"role": "user", "content": prompt})

    try:
        client = ZhipuAI(api_key=API_KEY)

        response = client.chat.completions.create(
            model=model_name,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )

        return {
            "success": True,
            "model": response.model,
            "content": response.choices[0].message.content,
            "usage": {
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            }
        }

    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

def main():
    """å‘½ä»¤è¡Œæ¥å£"""

    if len(sys.argv) < 2:
        print("=" * 80)
        print("GLM-4.7 é€šç”¨åŠ©æ‰‹")
        print("=" * 80)
        print()
        print("ç”¨æ³•ï¼š")
        print("  python3 glm47_helper.py \"ä½ çš„é—®é¢˜æˆ–ä»»åŠ¡\" [æ¨¡å‹]")
        print()
        print("æ¨¡å‹é€‰é¡¹ï¼š")
        print("  plus     - GLM-4.7 Plusï¼ˆæœ€æ–°æœ€å¼ºï¼Œé»˜è®¤ï¼‰")
        print("  flash    - GLM-4 Flashï¼ˆå¿«é€Ÿç»æµï¼‰")
        print("  air      - GLM-4 Airï¼ˆè½»é‡çº§ï¼‰")
        print("  standard - GLM-4ï¼ˆæ ‡å‡†ç‰ˆï¼‰")
        print()
        print("ç¤ºä¾‹ï¼š")
        print("  python3 glm47_helper.py \"å¸®æˆ‘å†™ä¸€ä¸ªPythonæ’åºç®—æ³•\"")
        print("  python3 glm47_helper.py \"ç‰™é«“ç‚çš„è¯Šæ–­æ ‡å‡†\" flash")
        print("  python3 glm47_helper.py \"åˆ†æè¿™ç»„æ•°æ®çš„ç»Ÿè®¡ç‰¹å¾\" plus")
        print()
        print("=" * 80)
        sys.exit(1)

    # è§£æå‚æ•°
    prompt = sys.argv[1]
    model = sys.argv[2] if len(sys.argv) > 2 else "plus"

    # æ˜¾ç¤ºä»»åŠ¡ä¿¡æ¯
    print("=" * 80)
    print(f"GLM-4.7 åŠ©æ‰‹ï¼ˆæ¨¡å‹ï¼š{model.upper()}ï¼‰")
    print("=" * 80)
    print(f"ä»»åŠ¡ï¼š{prompt}")
    print("-" * 80)
    print()

    # æ‰§è¡Œä»»åŠ¡
    result = chat_with_glm(prompt, model=model)

    # æ˜¾ç¤ºç»“æœ
    if result["success"]:
        print(result["content"])
        print()
        print("-" * 80)
        print(f"ğŸ“Š Token ä½¿ç”¨ï¼š{result['usage']['total_tokens']} "
              f"ï¼ˆè¾“å…¥ï¼š{result['usage']['prompt_tokens']}, "
              f"è¾“å‡ºï¼š{result['usage']['completion_tokens']}ï¼‰")
        print(f"âœ… æ¨¡å‹ï¼š{result['model']}")
    else:
        print(f"âŒ é”™è¯¯ï¼š{result['error']}")

    print("=" * 80)

if __name__ == "__main__":
    main()
