#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Claude Code å®æˆ˜è¯¾ç¨‹å†…å®¹æŠ“å–è„šæœ¬
æ‰¹é‡æŠ“å–æ‰€æœ‰ç« èŠ‚å†…å®¹å¹¶ä¿å­˜ä¸º Markdown æ–‡æ¡£
"""

import requests
from bs4 import BeautifulSoup
import json
import os
import time
from urllib.parse import urljoin

# åŸºç¡€ URL
BASE_URL = "https://cholf5.com/claude-code-in-action/"

# æ‰€æœ‰ç« èŠ‚åˆ—è¡¨
CHAPTERS = [
    "01-introduction.html",
    "02-what-is-a-coding-assistant.html",
    "03-claude-code-in-action.html",
    "04-claude-code-setup.html",
    "05-project-setup.html",
    "06-adding-context.html",
    "07-making-changes.html",
    "08-course-satisfaction-survey.html",
    "09-controlling-context.html",
    "10-custom-commands.html",
    "11-mcp-servers-with-claude-code.html",
    "12-github-integration.html",
    "13-introducing-hooks.html",
    "14-defining-hooks.html",
    "15-implementing-a-hook.html",
    "16-gotchas-around-hooks.html",
    "17-useful-hooks.html",
    "18-another-useful-hook.html",
    "19-the-claude-code-sdk.html",
    "20-quiz-on-claude-code.html",
    "21-summary-and-next-steps.html"
]

def fetch_page(url):
    """è·å–é¡µé¢å†…å®¹"""
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        response.encoding = 'utf-8'
        return response.text
    except Exception as e:
        print(f"âŒ è·å–å¤±è´¥: {url} - {str(e)}")
        return None

def extract_content(html, chapter_title):
    """ä» HTML ä¸­æå–ä¸»è¦å†…å®¹"""
    soup = BeautifulSoup(html, 'html.parser')

    # ç§»é™¤å¯¼èˆªæ ç­‰ä¸éœ€è¦çš„å†…å®¹
    for nav in soup.find_all('nav'):
        nav.decompose()
    for header in soup.find_all(['header', 'footer']):
        header.decompose()

    # æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
    content = soup.find('main') or soup.find('article') or soup.find('body')

    if not content:
        return None

    # æå–æ ‡é¢˜
    title_tag = content.find(['h1', 'h2'])
    title = title_tag.get_text().strip() if title_tag else chapter_title

    # è½¬æ¢ä¸º Markdown
    markdown_parts = []
    markdown_parts.append(f"\n\n# {title}\n\n")

    # å¤„ç†æ‰€æœ‰å­å…ƒç´ 
    for element in content.find_all(recursive=False):
        # è·³è¿‡æ ‡é¢˜ï¼ˆå·²ç»å¤„ç†è¿‡ï¼‰
        if element.name in ['h1', 'h2']:
            continue

        # å¤„ç†ä¸åŒçš„ HTML æ ‡ç­¾
        if element.name == 'h3':
            markdown_parts.append(f"\n## {element.get_text().strip()}\n")
        elif element.name == 'h4':
            markdown_parts.append(f"\n### {element.get_text().strip()}\n")
        elif element.name == 'p':
            text = element.get_text().strip()
            if text:
                markdown_parts.append(f"{text}\n\n")
        elif element.name == 'ul' or element.name == 'ol':
            items = element.find_all('li', recursive=False)
            for li in items:
                markdown_parts.append(f"- {li.get_text().strip()}\n")
            markdown_parts.append("\n")
        elif element.name == 'pre':
            code = element.get_text()
            markdown_parts.append(f"\n```\n{code}\n```\n\n")
        elif element.name == 'img':
            src = element.get('src', '')
            alt = element.get('alt', 'å›¾ç‰‡')
            markdown_parts.append(f"![{alt}]({src})\n\n")
        elif element.name == 'a':
            href = element.get('href', '')
            text = element.get_text().strip()
            markdown_parts.append(f"[{text}]({href})")

    return ''.join(markdown_parts)

def scrape_all_chapters():
    """æŠ“å–æ‰€æœ‰ç« èŠ‚"""
    all_content = []
    failed_chapters = []

    print("ğŸš€ å¼€å§‹æŠ“å– Claude Code å®æˆ˜è¯¾ç¨‹å†…å®¹...\n")

    for i, chapter_file in enumerate(CHAPTERS, 1):
        url = urljoin(BASE_URL, chapter_file)
        chapter_title = f"ç¬¬ {i:02d} èŠ‚: {chapter_file.replace('.html', '')}"

        print(f"ğŸ“– æ­£åœ¨æŠ“å– [{i}/{len(CHAPTERS)}]: {chapter_title}")

        html = fetch_page(url)
        if html:
            content = extract_content(html, chapter_title)
            if content:
                all_content.append(content)
                print(f"âœ… æˆåŠŸ: {chapter_title}\n")
            else:
                failed_chapters.append((url, "å†…å®¹æå–å¤±è´¥"))
                print(f"âš ï¸  å†…å®¹æå–å¤±è´¥: {chapter_title}\n")
        else:
            failed_chapters.append((url, "é¡µé¢è·å–å¤±è´¥"))
            print(f"âŒ å¤±è´¥: {chapter_title}\n")

        # é¿å…è¯·æ±‚è¿‡å¿«
        time.sleep(0.5)

    # ç”Ÿæˆå®Œæ•´çš„ Markdown æ–‡æ¡£
    full_markdown = "# Claude Code å®æˆ˜è¯¾ç¨‹ï¼ˆä¸­æ–‡ç¿»è¯‘ï¼‰\n\n"
    full_markdown += "> æœ¬æ–‡æ¡£ç”±è‡ªåŠ¨åŒ–è„šæœ¬æŠ“å–ç”Ÿæˆ\n"
    full_markdown += "> åŸè¯¾ç¨‹é“¾æ¥: https://anthropic.skilljar.com/claude-code-in-action/303233\n\n"
    full_markdown += "---\n\n"

    full_markdown += ''.join(all_content)

    # ä¿å­˜æ–‡ä»¶
    output_file = "/Users/lartemacfiles/Desktop/VPT-åˆè¯Šæ•°æ®/Claude_Codeå®æˆ˜è¯¾ç¨‹_å®Œæ•´ç‰ˆ.md"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(full_markdown)

    print(f"\nğŸ“„ Markdown æ–‡æ¡£å·²ä¿å­˜: {output_file}")

    # ç”ŸæˆæŠ¥å‘Š
    report = f"""# Claude Code å®æˆ˜è¯¾ç¨‹æŠ“å–æŠ¥å‘Š

## ç»Ÿè®¡ä¿¡æ¯
- æ€»ç« èŠ‚æ•°: {len(CHAPTERS)}
- æˆåŠŸæŠ“å–: {len(all_content)}
- å¤±è´¥ç« èŠ‚: {len(failed_chapters)}
- æˆåŠŸç‡: {len(all_content)/len(CHAPTERS)*100:.1f}%

## ç« èŠ‚åˆ—è¡¨
"""

    for i, chapter in enumerate(CHAPTERS, 1):
        report += f"{i}. {chapter}\n"

    if failed_chapters:
        report += "\n## å¤±è´¥ç« èŠ‚\n"
        for url, reason in failed_chapters:
            report += f"- âŒ {url} - {reason}\n"

    report += f"\n## è¾“å‡ºæ–‡ä»¶\n- Markdown: {output_file}\n\nç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}"

    report_file = "/Users/lartemacfiles/Desktop/VPT-åˆè¯Šæ•°æ®/å®æˆ˜è¯¾ç¨‹æŠ“å–æŠ¥å‘Š.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"ğŸ“Š æŠ“å–æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    print(f"\nâœ¨ æŠ“å–å®Œæˆ!")
    print(f"âœ… æˆåŠŸ: {len(all_content)} ç« ")
    if failed_chapters:
        print(f"âŒ å¤±è´¥: {len(failed_chapters)} ç« ")

if __name__ == "__main__":
    scrape_all_chapters()
